{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/Foreign%20word%20detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "ea0e1ebf-75f4-4a5a-be8e-43955d525913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/gdrive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "d2e2ebd7-b133-4d10-a9b9-3f6a628bca4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1Ikn75GANDVFyb0klJe4-EBQJikt4_Ob_/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Essential packages"
      ],
      "metadata": {
        "id": "njh0oGO4Nr7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "n2iCwQ7T3uBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the necessary libraries\n",
        "!pip install -q hazm\n",
        "!pip install -q dadmatools\n",
        "!pip install -q googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "znLvol8uYzV-",
        "outputId": "14071e5f-fb6c-4b3f-e68e-85b2ff6b7865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 KB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.4/390.4 KB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m495.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary libraries\n",
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "\n",
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating dataset"
      ],
      "metadata": {
        "id": "QU-8QbkZN4Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/corpus.txt\",\"rb\") as f:\n",
        "  corpo = pickle.load(f)\n",
        "\n",
        "corpo"
      ],
      "metadata": {
        "id": "wtvAazXLhkjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the corpus used for this task comes from hazm word list\n",
        "corpus = list(set(corpo))\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLtriJNNo35v",
        "outputId": "493d2bbd-42b2-4aed-b255-2ae14bcb94cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35316"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#roles contains all the possible roles in our dictionary\n",
        "roles = [x[2] for x in hazm.words_list()]\n",
        "roles = list(set(itertools.chain(*roles)))\n",
        "roles"
      ],
      "metadata": {
        "id": "febyfjwQr0hc",
        "outputId": "8828e36a-da3c-4a76-d78f-d868d34bcc00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PRO',\n",
              " 'RES',\n",
              " 'CL',\n",
              " 'N',\n",
              " 'PL',\n",
              " 'P',\n",
              " 'INT',\n",
              " 'NIN',\n",
              " 'AJC',\n",
              " 'PR',\n",
              " 'ADV',\n",
              " 'PS',\n",
              " 'V',\n",
              " 'AJ',\n",
              " 'ZVR',\n",
              " 'POSTP',\n",
              " 'COMP',\n",
              " 'NUM',\n",
              " 'CONJ',\n",
              " 'DET']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = hazm.Lemmatizer()\n",
        "stemmer = hazm.Stemmer()"
      ],
      "metadata": {
        "id": "LZ0cqll7Zwdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''this way of defining stop_words may not be usual,\n",
        "but the main purpose has been to eliminate disturbing words;\n",
        "such as: punctiation marks, pronouns and Persian\\Arabic grammatical signs'''\n",
        "\n",
        "add_list = [\"ند\",\"یم\",\"ید\",\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"ها\",\"ات\",\"ان\",\"ین\",\"ون\",\"‌های\"]\n",
        "stop_words = add_list\n",
        "stop_words.extend([\".\",\"؛\",\",\",\":\",\"?\",\"!\",\"؟\",\"ْ\",\"ٌ\",\"ٍ\",\"ً\",\"ُ\",\"ِ\",\"َ\",\"ّ\",\"}\",\"ؤ\",\"{\",\"»\",\"«\",\"ٰ\",\"‌‌‌‌‌‌‌‌ٔ\",\"ء\",\"\\\"\"])\n",
        "more_stopwords=[\"ها\",\"تر\",\"ترین\"]\n",
        "stop_words.extend(more_stopwords)"
      ],
      "metadata": {
        "id": "sUJqnvFEwe3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpu = []\n",
        "for word in corpus:\n",
        "  corpu.append(word)\n",
        "  corpu.append(lemmatizer.lemmatize(word))\n",
        "  for ele in add_list:\n",
        "    corpu.append(word+ele)\n",
        "\n",
        "len(corpu)"
      ],
      "metadata": {
        "id": "SlN7tsHwpAyJ",
        "outputId": "ed95d2f3-e7c8-43e4-e050-7001f2cd93c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1553904"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Config"
      ],
      "metadata": {
        "id": "uoztZzu6NAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp is the dadmatools' pipeline used for lemmatization purpose only\n",
        "pips = 'lem'\n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ShvxXYdXJ0Y",
        "outputId": "dc3e29dc-1a64-43be-8aa9-00a471ac4798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file /root/.pernlp/fa_tokenizer.pt: : 639kB [00:01, 493kB/s]                         \n",
            "Downloading file /root/.pernlp/fa_mwt.pt: : 721kB [00:01, 550kB/s]                           \n",
            "Downloading file /root/.pernlp/fa_lemmatizer.pt: : 4.69MB [00:01, 2.84MB/s]                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================= Pipeline Overview =============================\u001b[0m\n",
            "\n",
            "#   Component   Assigns       Requires   Scores   Retokenizes\n",
            "-   ---------   -----------   --------   ------   -----------\n",
            "0   tokenizer                                     True       \n",
            "                                                             \n",
            "1   lemmatize   token.lemma                       False      \n",
            "\n",
            "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
            "{'summary': {'tokenizer': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': True}, 'lemmatize': {'assigns': ['token.lemma'], 'requires': [], 'scores': [], 'retokenizes': False}}, 'problems': {'tokenizer': [], 'lemmatize': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatize'], 'requires': []}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we use an args dictionary to initialize our model\n",
        "args = {\n",
        "    'corpus':corpus,\n",
        "    'pipe':nlp,\n",
        "    'stop_words':stop_words,\n",
        "    'normalizer':hazm.Normalizer(),\n",
        "    'lemmatizer':hazm.Lemmatizer(),\n",
        "    'POS_tagger':hazm.POSTagger(model='Nlp HW2/resources/postagger.model'),\n",
        "    'translator':None,\n",
        "    'word_tokenizer':hazm.WordTokenizer(),\n",
        "    'sent_tokenizer':hazm.SentenceTokenizer()\n",
        "}"
      ],
      "metadata": {
        "id": "qP0mg4wDOMIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0AjCVFGFw2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "6kvuGlYaNPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#as the name suggests, the class is used to spot and show the positin of foreign words within the text\n",
        "\n",
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "\n",
        "    self.corpus = args['corpus']\n",
        "    self.stop_words = args['stop_words']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.lemmatizer = args['lemmatizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "    self.tagger = args['POS_tagger']\n",
        "\n",
        "#the following function is the main funtion of the model used for detecting the foreign words and their position\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "\n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    normalized = [sent.replace(\"\\u200cهای\",\"\") for sent in normalized]\n",
        "    normalized = [sent.replace(\"\\u200c\",\" \") for sent in normalized]\n",
        "    # print(f\"normalized:{normalized}\")\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in normalized]\n",
        "    # print(f\"lemmed:{lemmed}\")\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in lemmed]\n",
        "\n",
        "    final_words_list = []\n",
        "    for sent in words_list:\n",
        "      words_second_list = []\n",
        "      tags = self.tagger.tag(sent)\n",
        "      for i in range(len(tags)):\n",
        "        if tags[i][1] != \"V\":\n",
        "          words_second_list.append(tags[i][0])\n",
        "      final_words_list.append(words_second_list)\n",
        "\n",
        "    words_list = list(itertools.chain(*final_words_list))\n",
        "    words_list = [word for word in words_list if word not in self.stop_words]\n",
        "    #lemmed_words_list = [lemmatizer.lemmatize(word) for word in words_list]\n",
        "    # print(lemmed_words_list)\n",
        "\n",
        "    output = dict()\n",
        "    for word in words_list:\n",
        "      if word not in self.corpus:\n",
        "        output[word] = []\n",
        "        add_ind = 0\n",
        "\n",
        "        string = text\n",
        "        while word in string:\n",
        "\n",
        "          begin = string.find(word)\n",
        "          output[word].append((begin+add_ind,add_ind+begin+len(word)))\n",
        "\n",
        "          string = string[begin+len(word):]\n",
        "          add_ind+= begin+len(word)\n",
        "\n",
        "    return output\n",
        ""
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "Oyh-F0TrNX2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####results"
      ],
      "metadata": {
        "id": "REhKcO_FRmuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.load_csv(\"Nlp HW2/test.csv\")"
      ],
      "metadata": {
        "id": "l5ixR8F9R7fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####user run"
      ],
      "metadata": {
        "id": "tljaM-3WRscu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the run function, is an interactive way to map user_input to the detectors answer\n",
        "def run():\n",
        "  detector = foreign_word_detector(args)\n",
        "\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True :\n",
        "    text = input()\n",
        "    if text == \"\":\n",
        "      break\n",
        "\n",
        "    output = detector.detect(text)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "df7f8297-fff2-4894-959c-9dbd9a2c407c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "بیا بریم فودکورت بیف‌استراگانوف بخوریم\n",
            "{'فودکورت': [(9, 16)], 'بیف': [(17, 20)], 'استراگانوف': [(21, 31)]}\n",
            "مادرت چطور است؟\n",
            "{}\n",
            "پدرت خوه؟\n",
            "{'پدرت': [(0, 4)], 'خوه': [(5, 8)]}\n",
            "پدرت خوبه؟\n",
            "{'پدرت': [(0, 4)]}\n",
            "پدرش چطوره\n",
            "{'چطوره': [(5, 10)]}\n",
            "\\درش چطور است\n",
            "{'<UNK>در': []}\n",
            "پدرش چطور است؟\n",
            "{}\n",
            "مادرت چطوره.\n",
            "{}\n",
            "\\درش چطوره.\n",
            "{'<UNK>در': []}\n",
            "پدرش چظوره.\n",
            "{'چظوره': [(5, 10)]}\n",
            "پدرش چطوره.\n",
            "{}\n",
            "پدرش چطوره\n",
            "{'چطوره': [(5, 10)]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp(\"بابات چطوره\")"
      ],
      "metadata": {
        "id": "LaClSPKSL3M3",
        "outputId": "61330e1a-d970-42e7-ad76-5b71880d5031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "بابات چطوره "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"جدیداً\")"
      ],
      "metadata": {
        "id": "gbYlZCXhE0cN",
        "outputId": "e6ef7fb1-d2cd-442a-a2ee-2a3bd802e249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'جدیداً'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"جدیدا\" in corpus:\n",
        "  print(\"Asdf\")"
      ],
      "metadata": {
        "id": "fzxbEfK5E_rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##New idea"
      ],
      "metadata": {
        "id": "hr0gxCMLNfjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using phonetics"
      ],
      "metadata": {
        "id": "cNb5-GbmEOyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We import an English dictionary. Add the english words in \"words\" and the corresponding phonetics to \"phonics_list\".\n",
        "words = []\n",
        "phonics_list = []\n",
        "dict_file = open('Nlp HW2/cmudict.dict', 'r')\n",
        "with dict_file as f:\n",
        "    phonics = [line.rstrip('\\n') for line in f]\n",
        "    for p in phonics:\n",
        "        x = p.split(' ')\n",
        "        words.append(x[0])\n",
        "        phonics_list.append(' '.join(x[1:]))"
      ],
      "metadata": {
        "id": "_kzbejEP6GP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_unique = list(set(itertools.chain(*[ph.split(\" \") for ph in phonics_list])))\n",
        "phons_as_list = [ph.split(\" \") for ph in phonics_list]"
      ],
      "metadata": {
        "id": "zulC4Adu6upO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/phons_exchange.txt\",\"rb\") as file:\n",
        "  phons_dict = pickle.load(file)"
      ],
      "metadata": {
        "id": "vtwOxqizjC-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_dict[\"NG\"] = (\"ن\")\n",
        "phons_dict[\"TH\"] = (\"ت\")\n",
        "phons_dict[\"AO2\"] = phons_dict[\"AO1\"]\n",
        "phons_dict[\"IH2\"] = (\"ای\",\"ی\")\n",
        "phons_dict[\"IH0\"] = phons_dict[\"IH2\"]\n",
        "phons_dict[\"IH1\"] = phons_dict[\"IH0\"]"
      ],
      "metadata": {
        "id": "6_UTCNuVjWlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting our words to their persian correspondent with the use of phonetics in the \"persianized\"\n",
        "persianized_list = []\n",
        "persianized = []\n",
        "for x in phons_as_list:\n",
        "  word_list = []\n",
        "  count=0\n",
        "  for ph in x:\n",
        "    count+=1\n",
        "    if len(phons_dict[ph])>1:\n",
        "      if count==1:\n",
        "        word_list.append(phons_dict[ph][0])\n",
        "      else:\n",
        "        word_list.append(phons_dict[ph][1])\n",
        "    else:\n",
        "      word_list.append(phons_dict[ph][0])\n",
        "\n",
        "  persianized_list.append(word_list)\n",
        "  persianized.append(''.join(word_list))"
      ],
      "metadata": {
        "id": "TZQlhU9GjSJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking some words from \"words\"\n",
        "words[41000:41010]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_pSldPjknPo",
        "outputId": "c8ed37d6-a80c-466a-fd5a-e3892852af2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fancier',\n",
              " 'fanciers',\n",
              " 'fancies',\n",
              " 'fanciest',\n",
              " 'fanciful',\n",
              " 'fancy',\n",
              " 'fandango',\n",
              " 'fandel',\n",
              " 'fandrich',\n",
              " 'fane']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the corresponding persionized words from \"persionized\"\n",
        "persianized[41000:41010]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mKUNAPmkdt9",
        "outputId": "9cc33ed1-8aa0-438d-f5c0-fc531d9b185f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['فنسير',\n",
              " 'فنسيرز',\n",
              " 'فنسيز',\n",
              " 'فنسيست',\n",
              " 'فنسیفل',\n",
              " 'فنسي',\n",
              " 'فندنگو',\n",
              " 'فندل',\n",
              " 'فندریک',\n",
              " 'فین']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if our persionized method works fine on some words.\n",
        "listt = ['contribution','thanks','computer','system','task','time','garden','burger','chop','fandrich','infinity']\n",
        "\n",
        "for i in range(len(words)):\n",
        "  if words[i] in listt:\n",
        "    print(words[i])\n",
        "    print(phons_as_list[i])\n",
        "    print(i)\n",
        "    print(persianized[i])\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm2B0nXskxX8",
        "outputId": "242c8845-b936-4bc9-9a78-abd9c06dd4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "burger\n",
            "['B', 'ER1', 'G', 'ER0']\n",
            "16631\n",
            "برگر\n",
            "-----------------\n",
            "chop\n",
            "['CH', 'AA1', 'P']\n",
            "21283\n",
            "چاپ\n",
            "-----------------\n",
            "computer\n",
            "['K', 'AH0', 'M', 'P', 'Y', 'UW1', 'T', 'ER0']\n",
            "23974\n",
            "کمپیوتر\n",
            "-----------------\n",
            "contribution\n",
            "['K', 'AA2', 'N', 'T', 'R', 'AH0', 'B', 'Y', 'UW1', 'SH', 'AH0', 'N']\n",
            "25006\n",
            "کانتربیوشن\n",
            "-----------------\n",
            "fandrich\n",
            "['F', 'AE1', 'N', 'D', 'R', 'IH0', 'K']\n",
            "41008\n",
            "فندریک\n",
            "-----------------\n",
            "garden\n",
            "['G', 'AA1', 'R', 'D', 'AH0', 'N']\n",
            "46458\n",
            "گاردن\n",
            "-----------------\n",
            "infinity\n",
            "['IH2', 'N', 'F', 'IH1', 'N', 'IH0', 'T', 'IY0']\n",
            "59556\n",
            "اینفینیتي\n",
            "-----------------\n",
            "system\n",
            "['S', 'IH1', 'S', 'T', 'AH0', 'M']\n",
            "118902\n",
            "سیستم\n",
            "-----------------\n",
            "task\n",
            "['T', 'AE1', 'S', 'K']\n",
            "119638\n",
            "تسک\n",
            "-----------------\n",
            "thanks\n",
            "['TH', 'AE1', 'NG', 'K', 'S']\n",
            "120780\n",
            "تنکس\n",
            "-----------------\n",
            "time\n",
            "['T', 'AY1', 'M']\n",
            "121710\n",
            "تایم\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#phons_dict contains the Persian equivalent of English phonetics\n",
        "phons_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gz8wC5fCZrn",
        "outputId": "7498559d-a255-4dd7-cb13-01205eb4f1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EH0': ('ا', ''),\n",
              " 'ER2': ('ار', 'ر'),\n",
              " 'old': ('',),\n",
              " 'EY0': ('ای', 'ی'),\n",
              " 'IY2': ('ی',),\n",
              " 'name': ('',),\n",
              " 'S': ('س',),\n",
              " 'CH': ('چ',),\n",
              " 'K': ('ک',),\n",
              " 'UW2': ('یو',),\n",
              " 'AA0': ('آ', 'ا'),\n",
              " 'AY2': ('آی', 'ای'),\n",
              " 'OY0': ('اوی', 'وی'),\n",
              " 'AA1': ('آ', 'ا'),\n",
              " 'L': ('ل',),\n",
              " 'UH0': ('و',),\n",
              " 'B': ('ب',),\n",
              " 'F': ('ف',),\n",
              " 'OY1': ('اوی', 'وی'),\n",
              " 'abbrev': ('',),\n",
              " 'UH2': ('و',),\n",
              " 'D': ('د',),\n",
              " 'JH': ('ج',),\n",
              " 'W': ('و',),\n",
              " 'G': ('گ',),\n",
              " 'IH0': ('ای', 'ی'),\n",
              " 'UH1': ('و',),\n",
              " 'french': ('',),\n",
              " 'Z': ('ز',),\n",
              " 'AA2': ('آ', 'ا'),\n",
              " 'OY2': ('اوی', 'وی'),\n",
              " 'SH': ('ش',),\n",
              " 'AH0': ('ا', ''),\n",
              " 'foreign': ('',),\n",
              " 'UW0': ('یو',),\n",
              " 'AH1': ('ا', ''),\n",
              " 'IH1': ('ای', 'ی'),\n",
              " 'EY2': ('ای', 'ی'),\n",
              " 'AH2': ('ا', ''),\n",
              " 'OW2': ('او', 'و'),\n",
              " 'Y': ('ی',),\n",
              " 'AE2': ('',),\n",
              " 'DH': ('ض',),\n",
              " 'P': ('پ',),\n",
              " 'ER1': ('ار', 'ر'),\n",
              " 'AW2': ('او',),\n",
              " '#': ('',),\n",
              " 'EH1': ('ا', ''),\n",
              " 'AE1': ('',),\n",
              " 'EH2': ('ا', ''),\n",
              " 'N': ('ن',),\n",
              " 'ZH': ('ز',),\n",
              " 'OW1': ('او', 'و'),\n",
              " 'UW1': ('او', 'و'),\n",
              " 'AY0': ('آی', 'ای'),\n",
              " 'AY1': ('آی', 'ای'),\n",
              " 'AE0': ('',),\n",
              " 'NG': 'ن',\n",
              " 'AO1': ('او', 'و'),\n",
              " 'OW0': ('او', 'و'),\n",
              " 'HH': ('ه',),\n",
              " 'AW0': ('او',),\n",
              " 'TH': 'ت',\n",
              " 'IY0': ('ي',),\n",
              " 'AO0': ('او', 'و'),\n",
              " 'AW1': ('او',),\n",
              " 'R': ('ر',),\n",
              " 'AO2': ('او', 'و'),\n",
              " 'M': ('م',),\n",
              " 'T': ('ت',),\n",
              " 'IH2': ('ای', 'ی'),\n",
              " 'V': ('و',),\n",
              " 'ER0': ('ار', 'ر'),\n",
              " 'IY1': ('ی',),\n",
              " 'EY1': ('ای', 'ی')}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Translate and replace"
      ],
      "metadata": {
        "id": "sLE4BxvSEHIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_english_words(text):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "  detector = foreign_word_detector(args)\n",
        "  output = detector.detect(text)\n",
        "  print(f\"Output: {output}\")\n",
        "  english_text=text\n",
        "  new_text=text\n",
        "  for word in output.keys():\n",
        "    # first_index=output[word][0]\n",
        "    # second_index=output[word][1]\n",
        "    english_form=translator.translate(word).text\n",
        "    persian_translation=translator.translate(english_form, dest='fa').text\n",
        "    persian_translation=persian_translation.split(\"-\")[0]\n",
        "    new_text=new_text.replace(word,persian_translation)\n",
        "  return new_text,english_text"
      ],
      "metadata": {
        "id": "BWa2xvzrNhFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text= \"تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی تنکس .\"\n",
        "text= \"سلام من و فرزندت در گاردنمان همبرگر و استیک خوریدم و آن‌ ها خیلی دلیشز بودند .\"\n",
        "# text=\"من و فرندم نشستیم گیم آف ترونز دیدیم ولی اصلا به گرد پای برکینگ بد هم نمیرسید.\"\n",
        "translation,english_text=change_english_words(text)\n",
        "print(translation)\n",
        "# print(english_text)"
      ],
      "metadata": {
        "id": "qb3F74t0DUMA",
        "outputId": "31f8c043-28dd-4c4f-b274-00708b49989c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: {'گاردن': [(20, 25)], 'دلیشز': [(65, 70)]}\n",
            "سلام من و فرزندت در باغمان همبرگر و استیک خوریدم و آن‌ ها خیلی خوشمزه  بودند .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bf3PwdfqDvwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}