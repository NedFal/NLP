{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/Foreign%20word%20detector(with%20indirect%20translating).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "91d9be72-955f-4a4b-e6e3-0207141ee663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "beaf2850-b0da-4ff7-8e45-c6c5d6646df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1Ikn75GANDVFyb0klJe4-EBQJikt4_Ob_/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Essential packages"
      ],
      "metadata": {
        "id": "njh0oGO4Nr7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "n2iCwQ7T3uBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the necessary libraries\n",
        "!pip install -q hazm\n",
        "!pip install -q dadmatools\n",
        "!pip install -q googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "znLvol8uYzV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd136cfd-0fea-42ef-9e1e-f6b470975492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 KB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.4/390.4 KB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary libraries\n",
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "\n",
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating dataset"
      ],
      "metadata": {
        "id": "QU-8QbkZN4Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/corpus.txt\",\"rb\") as f:\n",
        "  corpo = pickle.load(f)\n",
        "\n",
        "corpo"
      ],
      "metadata": {
        "id": "wtvAazXLhkjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the corpus used for this task comes from hazm word list\n",
        "corpus = list(set(corpo))\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLtriJNNo35v",
        "outputId": "0bad031b-a161-44f6-b436-6fe577a9c735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35316"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#roles contains all the possible roles in our dictionary\n",
        "roles = [x[2] for x in hazm.words_list()]\n",
        "roles = list(set(itertools.chain(*roles)))\n",
        "roles"
      ],
      "metadata": {
        "id": "febyfjwQr0hc",
        "outputId": "e9641993-7c63-4216-ba21-239211b5df27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DET',\n",
              " 'NIN',\n",
              " 'PL',\n",
              " 'PR',\n",
              " 'AJ',\n",
              " 'N',\n",
              " 'CONJ',\n",
              " 'COMP',\n",
              " 'PS',\n",
              " 'AJC',\n",
              " 'ADV',\n",
              " 'INT',\n",
              " 'CL',\n",
              " 'PRO',\n",
              " 'RES',\n",
              " 'V',\n",
              " 'P',\n",
              " 'ZVR',\n",
              " 'POSTP',\n",
              " 'NUM']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = hazm.Lemmatizer()\n",
        "stemmer = hazm.Stemmer()"
      ],
      "metadata": {
        "id": "LZ0cqll7Zwdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''this way of defining stop_words may not be usual,\n",
        "but the main purpose has been to eliminate disturbing words;\n",
        "such as: punctiation marks, pronouns and Persian\\Arabic grammatical signs'''\n",
        "\n",
        "stop_words = [\"ند\",\"یم\",\"ید\",\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"ها\",\"ات\",\"ان\",\"ین\",\"ون\",\"‌های\"]\n",
        "stop_words.extend([\".\",\"؛\",\",\",\":\",\"?\",\"!\",\"؟\",\"ْ\",\"ٌ\",\"ٍ\",\"ً\",\"ُ\",\"ِ\",\"َ\",\"ّ\",\"}\",\"ؤ\",\"{\",\"»\",\"«\",\"ٰ\",\"‌‌‌‌‌‌‌‌ٔ\",\"ء\",\"\\\"\"])\n",
        "more_stopwords=[\"ها\",\"تر\",\"ترین\"]\n",
        "stop_words.extend(more_stopwords)"
      ],
      "metadata": {
        "id": "sUJqnvFEwe3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_list = [\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"‌ها\"]\n",
        "corpu = []\n",
        "for word in hazm.words_list():\n",
        "  corpu.append(word[0])\n",
        "  corpu.append(lemmatizer.lemmatize(word[0]))\n",
        "  if (\"N\" in word[2]) or (\"AJC\" in word[2]) or (\"AJ\" in word[2]):\n",
        "    for ele in add_list:\n",
        "      corpu.append(word[0]+ele)\n",
        "\n",
        "corpu = list(set(corpu))\n",
        "len(corpu)"
      ],
      "metadata": {
        "id": "SlN7tsHwpAyJ",
        "outputId": "9ef18230-c60b-4c03-917d-72d2ef572089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308048"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Config"
      ],
      "metadata": {
        "id": "uoztZzu6NAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp is the dadmatools' pipeline used for lemmatization purpose only\n",
        "pips = 'lem'\n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ShvxXYdXJ0Y",
        "outputId": "de187544-c8bc-49ee-b403-c08747954805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file /root/.pernlp/fa_tokenizer.pt: : 639kB [00:01, 532kB/s]                           \n",
            "Downloading file /root/.pernlp/fa_mwt.pt: : 721kB [00:01, 582kB/s]                           \n",
            "Downloading file /root/.pernlp/fa_lemmatizer.pt: : 4.69MB [00:01, 3.40MB/s]                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================= Pipeline Overview =============================\u001b[0m\n",
            "\n",
            "#   Component   Assigns       Requires   Scores   Retokenizes\n",
            "-   ---------   -----------   --------   ------   -----------\n",
            "0   tokenizer                                     True       \n",
            "                                                             \n",
            "1   lemmatize   token.lemma                       False      \n",
            "\n",
            "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
            "{'summary': {'tokenizer': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': True}, 'lemmatize': {'assigns': ['token.lemma'], 'requires': [], 'scores': [], 'retokenizes': False}}, 'problems': {'tokenizer': [], 'lemmatize': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatize'], 'requires': []}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we use an args dictionary to initialize our model\n",
        "args = {\n",
        "    'corpus':corpu,\n",
        "    'pipe':nlp,\n",
        "    'stop_words':stop_words,\n",
        "    'normalizer':hazm.Normalizer(),\n",
        "    'lemmatizer':hazm.Lemmatizer(),\n",
        "    'POS_tagger':hazm.POSTagger(model='Nlp HW2/resources/postagger.model'),\n",
        "    'translator':None,\n",
        "    'word_tokenizer':hazm.WordTokenizer(),\n",
        "    'sent_tokenizer':hazm.SentenceTokenizer()\n",
        "}"
      ],
      "metadata": {
        "id": "qP0mg4wDOMIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "6kvuGlYaNPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#as the name suggests, the class is used to spot and show the positin of foreign words within the text\n",
        "\n",
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "\n",
        "    self.corpus = args['corpus']\n",
        "    self.stop_words = args['stop_words']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.lemmatizer = args['lemmatizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "    self.tagger = args['POS_tagger']\n",
        "\n",
        "#the following function is the main funtion of the model used for detecting the foreign words and their position\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "\n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    normalized = [sent.replace(\"\\u200cهای\",\"\") for sent in normalized]\n",
        "    normalized = [sent.replace(\"\\u200c\",\" \") for sent in normalized]\n",
        "    # print(f\"normalized:{normalized}\")\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in normalized]\n",
        "    # print(f\"lemmed:{lemmed}\")\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in lemmed]\n",
        "\n",
        "    final_words_list = []\n",
        "    for sent in words_list:\n",
        "      words_second_list = []\n",
        "      tags = self.tagger.tag(sent)\n",
        "      for i in range(len(tags)):\n",
        "        if tags[i][1] != \"V\":\n",
        "          words_second_list.append(tags[i][0])\n",
        "      final_words_list.append(words_second_list)\n",
        "\n",
        "    words_list = list(itertools.chain(*final_words_list))\n",
        "    words_list = [word for word in words_list if word not in self.stop_words]\n",
        "    #lemmed_words_list = [lemmatizer.lemmatize(word) for word in words_list]\n",
        "    # print(lemmed_words_list)\n",
        "\n",
        "    output = dict()\n",
        "    for word in words_list:\n",
        "      if word not in self.corpus:\n",
        "        output[word] = []\n",
        "        add_ind = 0\n",
        "\n",
        "        string = text\n",
        "        while word in string:\n",
        "\n",
        "          begin = string.find(word)\n",
        "          output[word].append((begin+add_ind,add_ind+begin+len(word)))\n",
        "\n",
        "          string = string[begin+len(word):]\n",
        "          add_ind+= begin+len(word)\n",
        "\n",
        "    return output\n",
        ""
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "Oyh-F0TrNX2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####results"
      ],
      "metadata": {
        "id": "REhKcO_FRmuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.load_csv(\"Nlp HW2/test.xlsx\")\n",
        "texts = pickle.load(open(\"Nlp HW2/data_text.TXT\"))\n",
        "\n",
        "#join the 2 parts into 1 dataframe\n",
        "\n",
        "#create an eng_words coloumn"
      ],
      "metadata": {
        "id": "l5ixR8F9R7fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####user run"
      ],
      "metadata": {
        "id": "tljaM-3WRscu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the run function, is an interactive way to map user_input to the detectors answer\n",
        "def run():\n",
        "  detector = foreign_word_detector(args)\n",
        "\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True :\n",
        "    text = input()\n",
        "    if text == \"\":\n",
        "      break\n",
        "\n",
        "    output = detector.detect(text)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "609940b7-c7f1-42f7-c35b-6d97ef5db8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "پدرت چطوره؟\n",
            "{}\n",
            "مادرت چطوره؟\n",
            "{}\n",
            "تایم داری؟\n",
            "{}\n",
            "امروز تایم داری بریم بیرون؟\n",
            "{'تایم': [(6, 10)]}\n",
            "امروز یک کار خیلی هاردی داشتیم اما تو کانتریبیوشن خوبی داشتی؛ تنکس.\n",
            "{'کانتریبیوشن': [(38, 49)], 'تنکس': [(62, 66)]}\n",
            "سیستم کامپیوترم خراب شده‌است.\n",
            "{'سیستم': [(0, 5)], 'کامپیوتر': [(6, 14)]}\n",
            "من و علی در گاردنمان قدم زدیم.\n",
            "{'گاردن': [(12, 17)]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##New idea"
      ],
      "metadata": {
        "id": "hr0gxCMLNfjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using phonetics"
      ],
      "metadata": {
        "id": "cNb5-GbmEOyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We import an English dictionary. Add the english words in \"words\" and the corresponding phonetics to \"phonics_list\".\n",
        "words = []\n",
        "phonics_list = []\n",
        "dict_file = open('Nlp HW2/cmudict.dict', 'r')\n",
        "with dict_file as f:\n",
        "    phonics = [line.rstrip('\\n') for line in f]\n",
        "    for p in phonics:\n",
        "        x = p.split(' ')\n",
        "        words.append(x[0])\n",
        "        phonics_list.append(' '.join(x[1:]))"
      ],
      "metadata": {
        "id": "_kzbejEP6GP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_unique = list(set(itertools.chain(*[ph.split(\" \") for ph in phonics_list])))\n",
        "phons_as_list = [ph.split(\" \") for ph in phonics_list]"
      ],
      "metadata": {
        "id": "zulC4Adu6upO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/phons_exchange.txt\",\"rb\") as file:\n",
        "  phons_dict = pickle.load(file)"
      ],
      "metadata": {
        "id": "vtwOxqizjC-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_dict[\"NG\"] = (\"ن\")\n",
        "phons_dict[\"TH\"] = (\"ت\")\n",
        "phons_dict[\"AO2\"] = phons_dict[\"AO1\"]\n",
        "phons_dict[\"IH2\"] = (\"ای\",\"ی\")\n",
        "phons_dict[\"IH0\"] = phons_dict[\"IH2\"]\n",
        "phons_dict[\"IH1\"] = phons_dict[\"IH0\"]"
      ],
      "metadata": {
        "id": "6_UTCNuVjWlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting our words to their persian correspondent with the use of phonetics in the \"persianized\"\n",
        "persianized_list = []\n",
        "persianized = []\n",
        "for x in phons_as_list:\n",
        "  word_list = []\n",
        "  count=0\n",
        "  for ph in x:\n",
        "    count+=1\n",
        "    if len(phons_dict[ph])>1:\n",
        "      if count==1:\n",
        "        word_list.append(phons_dict[ph][0])\n",
        "      else:\n",
        "        word_list.append(phons_dict[ph][1])\n",
        "    else:\n",
        "      word_list.append(phons_dict[ph][0])\n",
        "\n",
        "  persianized_list.append(word_list)\n",
        "  persianized.append(''.join(word_list))"
      ],
      "metadata": {
        "id": "TZQlhU9GjSJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking some words from \"words\"\n",
        "words[41000:41010]"
      ],
      "metadata": {
        "id": "5_pSldPjknPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the corresponding persionized words from \"persionized\"\n",
        "persianized[41000:41010]"
      ],
      "metadata": {
        "id": "4mKUNAPmkdt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if our persionized method works fine on some words.\n",
        "listt = ['contribution','thanks','computer','system','task','time','garden','burger','chop','fandrich','infinity']\n",
        "\n",
        "for i in range(len(words)):\n",
        "  if words[i] in listt:\n",
        "    print(words[i])\n",
        "    print(phons_as_list[i])\n",
        "    print(i)\n",
        "    print(persianized[i])\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "id": "Pm2B0nXskxX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#phons_dict contains the Persian equivalent of English phonetics\n",
        "phons_dict"
      ],
      "metadata": {
        "id": "4Gz8wC5fCZrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Translate and replace"
      ],
      "metadata": {
        "id": "sLE4BxvSEHIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_english_words(text):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "  detector = foreign_word_detector(args)\n",
        "  output = detector.detect(text)\n",
        "  print(f\"Output: {output}\")\n",
        "  english_text=text\n",
        "  new_text=text\n",
        "  for word in output.keys():\n",
        "    # first_index=output[word][0]\n",
        "    # second_index=output[word][1]\n",
        "    english_form=translator.translate(word).text\n",
        "    persian_translation=translator.translate(english_form, dest='fa').text\n",
        "    persian_translation=persian_translation.split(\"-\")[0]\n",
        "    new_text=new_text.replace(word,persian_translation)\n",
        "  return new_text,english_text"
      ],
      "metadata": {
        "id": "BWa2xvzrNhFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text= \"تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی تنکس .\"\n",
        "text= \"سلام من و فرزندت در گاردنمان همبرگر و استیک خوریدم و آن‌ ها خیلی دلیشز بودند .\"\n",
        "# text=\"من و فرندم نشستیم گیم آف ترونز دیدیم ولی اصلا به گرد پای برکینگ بد هم نمیرسید.\"\n",
        "translation,english_text=change_english_words(text)\n",
        "print(translation)\n",
        "# print(english_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb3F74t0DUMA",
        "outputId": "fb8b9598-ce5c-4afa-c9f3-f9dc537bab1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: {'گاردن': [(20, 25)], 'دلیشز': [(65, 70)]}\n",
            "سلام من و فرزندت در باغمان همبرگر و استیک خوریدم و آن‌ ها خیلی خوشمزه  بودند .\n"
          ]
        }
      ]
    }
  ]
}
